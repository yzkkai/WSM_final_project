# WSM Final Project Sample Code

This repository simulates the execution workflow of the online judge system, including running inference, validating outputs, and generating detailed logs.

## Overview

The `run.sh` script performs the following tasks:

1. Runs inference using `My_RAG/main.py` for a given language and generate output files.
2. Validates the output format using `check_output_format.py`.
3. Logs every step with timestamps for easier debugging.

## Script Details

### 1. Inference

For each language (`en`, `zh`), the script calls:

```bash
python ./My_RAG/main.py \
    --query_path <query_file> \
    --docs_path <docs_file> \
    --language <lang> \
    --output <output_file>
```

- Input format (`query_file` in `--query_path`):

    Each line in the input file must be a valid JSON object with the following structure:

    | Field                   | Type    | Description                                      |
    | ----------------------- | ------- | ------------------------------------------------ |
    | `query.query_id`        | integer | Unique ID of the query                           |
    | `query.content`         | string  | The natural language question                    |
    | `prediction.content`    | string  | Initially empty, your answer will be filled here     |
    | `prediction.references` | list[string]    | Initially empty, your answer will be filled here |

    Example input line:

    ```json
    {"query": {"query_id": 2134, "content": "When did Green Fields Agriculture Ltd. appoint a new CEO?"}, "prediction": {"content": "", "references": []}}
    ```

- Output format (`output_file` in `--output`):

    Filled in your answer in `prediction.content` and `prediction.references` without changing other fields.

    Example output line:

    ```json
    {"query": {"query_id": 2134, "content": "When did Green Fields Agriculture Ltd. appoint a new CEO?"}, "prediction": {"content": "I don't know when Green Fields Agriculture Ltd. appointed a new CEO based on the provided context.", "references": ["Green Fields Agriculture Ltd., established on April 1, 2005 in Sunnydale, California, is a listed company on NASDAQ and specializes in the cultivation and distribution of high-quality organic fruits and vegetables.\nIn January 2021, Green Fields Agriculture Ltd. made a significant equity acquisition by acquiring 40% equity of Green Harvest Farm. This acquisition expanded their market share and enhanced their control over the industry. As part of this acquisition, Green Fields Agriculture Ltd. also purchased an additional 500 acres of farmland, expanding their production capacity and allowing them to cultivate a wider variety of organic fruits and vegetables. The company further invested in the refurbishment of their facilities in February 2021, resulting in improved operational efficiency, reduced water usage, and compliance with organic farming standards.\nIn March 2021, Green Fields Agriculture Ltd. entered into a strategic partnership with an agricultural research institution for prod"]}}
    ```

### 2. Output Format Check

```bash
python ./check_output_format.py \
    --query_file <query_file> \
    --processed_file <processed_file>
```

- `query_file`: The original input file used for inference.  
- `processed_file`: The output file generated by the inference step.

The script checks if the `processed_file` adheres to the expected output format and answer all queries in the `query_file`.


If the output format is correct, the script prints:

Format check passed.

## Configuration Settings

The system automatically loads the Ollama configuration from one of two files:

- `configs/config_local.yaml` – used for local development
- `configs/config_submit.yaml` – used when the project is submitted (because `configs/config_local.yaml` is ignored by Git)

When the program starts, it searches for the configuration files in order:

```python
config_paths = [
    configs_folder / "config_local.yaml",
    configs_folder / "config_submit.yaml",
]
```

If `config_local.yaml` exists, it takes priority and will be used. If it is not found (e.g., on the grading server), the system automatically falls back to `config_submit.yaml`.

This design allows you to use different configurations for local testing and for the submitted version.
You can freely adjust `config_local.yaml` during development for convenience, while the grading system will automatically use `config_submit.yaml`, ensuring that your submission always follows the required environment and settings.

## How to Run

```bash
pip install -r requirements.txt
./run.sh
```